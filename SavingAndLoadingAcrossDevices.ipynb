{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHnpzOgAQt9mX5A7oFniBd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renan-Domingues/PyTorchRecipes/blob/main/SavingAndLoadingAcrossDevices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and loading models across devices in PyTorch\n",
        "\n",
        "There may be instances where I want to save and load your neural networks across different devices."
      ],
      "metadata": {
        "id": "C_01C2fayKVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Saving and loading models across devices is relatively straightforward\n",
        "\n",
        "We'll saving and loading models across CPUs and GPUs."
      ],
      "metadata": {
        "id": "wOyUkuNjyV8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps\n",
        "\n",
        "1. Import all necessary libraries for loading our data\n",
        "2. Define and initialize the neural network\n",
        "3. Save on a GPU, load on a CPU\n",
        "4. Save on a GPU, load on a GPU\n",
        "5. Save on a CPU, load on a GPU\n",
        "6. Saving and loading DataParallel models\n"
      ],
      "metadata": {
        "id": "B_-J1-IHylAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import the libraries"
      ],
      "metadata": {
        "id": "iQDsklHryzJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "pQVsRuxcy3rt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Define and initialize the neural network"
      ],
      "metadata": {
        "id": "OC3qPc8wy_IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(x, self):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 16 * 5 * 5)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxmtZr9dzGNl",
        "outputId": "89397512-62ae-48ea-ed17-4748b676e19a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Save on GPU, Load on CPU\n",
        "\n",
        "When loading a model on a CPU that was trained with a GPU, pass torch.device('cpu') to the map_location argument in the torch.load() function."
      ],
      "metadata": {
        "id": "bl_z-qtx1B8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify a path to save to\n",
        "\n",
        "PATH = \"model.pt\"\n",
        "\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# Load\n",
        "device = torch.device('cpu')\n",
        "model = Net()\n",
        "model.load_state_dict(torch.load(PATH, map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5_EmGWS1PZO",
        "outputId": "2ea9cd55-7e46-41ce-ce68-3f7f634416e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the storages underlying the tensors are dynamically remapped to the CPU device using the map_location argument."
      ],
      "metadata": {
        "id": "GwJeF5iB159n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Save on GPU, Load on GPU\n",
        "\n",
        "When loading a model on a GPU that was trained and saved on GPU, simply convert the initialized model to a CUDA optimized model using model.to(torch.device('cuda')).\n",
        "\n",
        "Be sure to use the .to(torch.device('cuda')) function on all model inputs to prepare the data for the model."
      ],
      "metadata": {
        "id": "7aI3WBr216vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# Load\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = Net()\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "1r3zYZGJ6mWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### NOTE\n",
        "\n",
        "it's not working in my Pc because I'm not using NVIDIA driver sistem"
      ],
      "metadata": {
        "id": "TqnzspJm3ddq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that calling my_tensor.to(device) returns a new copy of my_tensor on GPU. It does NOT overwrite my_tensor. Therefore, remember to manually overwrite tensors: my_tensor = my_tensor.to(torch.device('cuda'))."
      ],
      "metadata": {
        "id": "Ixeewm9i3uoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Save on CPU, Load on GPU\n",
        "\n",
        "When loading a model on a GPU that was trained and saved on CPU, set the map_location argument in the torch.load() function to cuda:device_id. This loads the model to a given GPU device."
      ],
      "metadata": {
        "id": "Hya96rDh3yBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save\n",
        "\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# Load\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = Net()\n",
        "\n",
        "# Choose GPU device number\n",
        "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "CLJ8Dn064ADj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### NOTE\n",
        "Because I'm not using NVIDEA drive, tris code will not going to work either"
      ],
      "metadata": {
        "id": "0Ri90PQU4nCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Saving torch.nn.DataParallel Models\n",
        "\n",
        "torch.nn.DataParallel is a model wrapper that enables parallel GPU utilization.\n",
        "\n",
        "To save a DataParallel model generically, save the model.module.state_dict(). This way, I have the flexibility to load the model any way I want to any device I want."
      ],
      "metadata": {
        "id": "5EagqFGI41wu"
      }
    }
  ]
}