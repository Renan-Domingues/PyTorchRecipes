{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcGQuMtU0GYPhzUKgBnnJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renan-Domingues/PyTorchRecipes/blob/main/ReasoningAboutShapes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning about Shapes in PyTorch\n",
        "\n",
        "When writing models with PyTorch, it is commonly the case that the parameters to a given layer depend on the shape of the output of the previous layer. For example, the in_features of an nn.Linear layer must match the size(-1) of the input. For some layers, the shape computation involves complex equations, for example convolution operations\n",
        "\n",
        "One way around this is to run the forward pass with random inputs, but this is a waste of memory and compute\n",
        "\n",
        "We can use meta device instead to determine the output shapes of a layer without materializing any data."
      ],
      "metadata": {
        "id": "nd-e2VWpssnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import timeit\n",
        "\n",
        "t = torch.rand(2, 3, 10, 10, device=\"meta\")\n",
        "conv = torch.nn.Conv2d(3, 5, 2, device=\"meta\")\n",
        "start = timeit.default_timer()\n",
        "out = conv(t)\n",
        "end = timeit.default_timer()\n",
        "\n",
        "print(out)\n",
        "print(f\"Time taken: {end-start}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_Wp9jAOtRrP",
        "outputId": "5fb76883-d76b-4014-8ea7-949ca0e5162b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(..., device='meta', size=(2, 5, 9, 9), grad_fn=<ConvolutionBackward0>)\n",
            "Time taken: 0.0057404169997425925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since data is not materialized, passing arbitrarily large inputs will not significantly alter the time taken for shape computation."
      ],
      "metadata": {
        "id": "1dCF793luA0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_large = torch.rand(2**10, 3, 2**16, 2**16, device=\"meta\")\n",
        "start = timeit.default_timer()\n",
        "out = conv(t_large)\n",
        "end = timeit.default_timer()\n",
        "\n",
        "print(out)\n",
        "print(f\"Time taken: {end - start}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NTrBZ2-uKNh",
        "outputId": "5cf26e7c-64ba-4ab8-8361-e548442b1b4f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(..., device='meta', size=(1024, 5, 65535, 65535),\n",
            "       grad_fn=<ConvolutionBackward0>)\n",
            "Time taken: 0.0004833450002479367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a neural network"
      ],
      "metadata": {
        "id": "XrTEGMoGurxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "3-q4yL1guv4_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can view the intermediate shapes within an entire network by registering a forward hook to each layer that prints the shape of the output."
      ],
      "metadata": {
        "id": "6iYz-f60wYIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fw_hook(module, input, output):\n",
        "  print(f\"Shape of output to {module} is {output.shape}.\")\n",
        "\n",
        "# any tensor created within this torch.device context manager will be on the meta device.\n",
        "\n",
        "with torch.device(\"meta\"):\n",
        "  net = Net()\n",
        "  inp = torch.randn((1024, 3, 32, 32))\n",
        "\n",
        "for name, layer in net.named_modules():\n",
        "  layer.register_forward_hook(fw_hook)\n",
        "\n",
        "out = net(inp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRntBKeOwg0Z",
        "outputId": "2c59bcd9-75a7-44da-82d9-b9f8063ea823"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of output to Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1)) is torch.Size([1024, 6, 28, 28]).\n",
            "Shape of output to MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) is torch.Size([1024, 6, 14, 14]).\n",
            "Shape of output to Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) is torch.Size([1024, 16, 10, 10]).\n",
            "Shape of output to MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) is torch.Size([1024, 16, 5, 5]).\n",
            "Shape of output to Linear(in_features=400, out_features=120, bias=True) is torch.Size([1024, 120]).\n",
            "Shape of output to Linear(in_features=120, out_features=84, bias=True) is torch.Size([1024, 84]).\n",
            "Shape of output to Linear(in_features=84, out_features=10, bias=True) is torch.Size([1024, 10]).\n",
            "Shape of output to Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ") is torch.Size([1024, 10]).\n"
          ]
        }
      ]
    }
  ]
}